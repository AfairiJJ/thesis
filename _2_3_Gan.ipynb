{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# %%\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This script runs the multi GAN and allows you to step through each part\n",
    "# divide y by exposure in xpxixpy\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "# import modules\n",
    "import torch.optim as optim\n",
    "from patsy import dmatrices\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "## Import created modules\n",
    "from Functions.MC_WGAN_GP.gan_scripts.auto_loader import PolicyDataset\n",
    "from Functions.MC_WGAN_GP.gan_scripts.generator2_v2 import Generator2\n",
    "from Functions.MC_WGAN_GP.gan_scripts.discriminator2_v3 import Discriminator2\n",
    "from Functions.MC_WGAN_GP.gan_scripts.gradiant_penalty import calculate_gradient_penalty\n",
    "from Functions.MC_WGAN_GP.gan_scripts.undo_dummy import back_from_dummies\n",
    "\n",
    "from torch.autograd.variable import Variable\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-05-09T21:00:56.132886900Z",
     "start_time": "2023-05-09T21:00:50.299916200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "       VehPower    VehAge   DrivAge   Density  BonusMalus  Exposure  \\\ncount 487648.00 487648.00 487648.00 487648.00   487648.00 487648.00   \nmean       0.22      0.35      0.38      0.07        0.05      0.53   \nstd        0.19      0.27      0.20      0.15        0.09      0.36   \nmin        0.00      0.00      0.00      0.00        0.00      0.00   \n25%        0.09      0.10      0.22      0.00        0.00      0.18   \n50%        0.18      0.30      0.36      0.01        0.00      0.49   \n75%        0.27      0.55      0.51      0.06        0.08      0.99   \nmax        1.00      1.00      1.00      1.00        1.00      1.00   \n\n       ClaimNb_0.0  ClaimNb_1.0  ClaimNb_2.0  ClaimNb_3.0  ...  Region_R83  \\\ncount    487648.00    487648.00    487648.00    487648.00  ...   487648.00   \nmean          0.95         0.05         0.00         0.00  ...        0.01   \nstd           0.22         0.21         0.05         0.01  ...        0.09   \nmin           0.00         0.00         0.00         0.00  ...        0.00   \n25%           1.00         0.00         0.00         0.00  ...        0.00   \n50%           1.00         0.00         0.00         0.00  ...        0.00   \n75%           1.00         0.00         0.00         0.00  ...        0.00   \nmax           1.00         1.00         1.00         1.00  ...        1.00   \n\n       Region_R91  Region_R93  Region_R94    Area_A    Area_B    Area_C  \\\ncount   487648.00   487648.00   487648.00 487648.00 487648.00 487648.00   \nmean         0.05        0.12        0.01      0.15      0.11      0.28   \nstd          0.22        0.32        0.08      0.36      0.32      0.45   \nmin          0.00        0.00        0.00      0.00      0.00      0.00   \n25%          0.00        0.00        0.00      0.00      0.00      0.00   \n50%          0.00        0.00        0.00      0.00      0.00      0.00   \n75%          0.00        0.00        0.00      0.00      0.00      1.00   \nmax          1.00        1.00        1.00      1.00      1.00      1.00   \n\n         Area_D    Area_E    Area_F  \ncount 487648.00 487648.00 487648.00  \nmean       0.22      0.20      0.03  \nstd        0.42      0.40      0.16  \nmin        0.00      0.00      0.00  \n25%        0.00      0.00      0.00  \n50%        0.00      0.00      0.00  \n75%        0.00      0.00      0.00  \nmax        1.00      1.00      1.00  \n\n[8 rows x 52 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>VehPower</th>\n      <th>VehAge</th>\n      <th>DrivAge</th>\n      <th>Density</th>\n      <th>BonusMalus</th>\n      <th>Exposure</th>\n      <th>ClaimNb_0.0</th>\n      <th>ClaimNb_1.0</th>\n      <th>ClaimNb_2.0</th>\n      <th>ClaimNb_3.0</th>\n      <th>...</th>\n      <th>Region_R83</th>\n      <th>Region_R91</th>\n      <th>Region_R93</th>\n      <th>Region_R94</th>\n      <th>Area_A</th>\n      <th>Area_B</th>\n      <th>Area_C</th>\n      <th>Area_D</th>\n      <th>Area_E</th>\n      <th>Area_F</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>487648.00</td>\n      <td>487648.00</td>\n      <td>487648.00</td>\n      <td>487648.00</td>\n      <td>487648.00</td>\n      <td>487648.00</td>\n      <td>487648.00</td>\n      <td>487648.00</td>\n      <td>487648.00</td>\n      <td>487648.00</td>\n      <td>...</td>\n      <td>487648.00</td>\n      <td>487648.00</td>\n      <td>487648.00</td>\n      <td>487648.00</td>\n      <td>487648.00</td>\n      <td>487648.00</td>\n      <td>487648.00</td>\n      <td>487648.00</td>\n      <td>487648.00</td>\n      <td>487648.00</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.22</td>\n      <td>0.35</td>\n      <td>0.38</td>\n      <td>0.07</td>\n      <td>0.05</td>\n      <td>0.53</td>\n      <td>0.95</td>\n      <td>0.05</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>...</td>\n      <td>0.01</td>\n      <td>0.05</td>\n      <td>0.12</td>\n      <td>0.01</td>\n      <td>0.15</td>\n      <td>0.11</td>\n      <td>0.28</td>\n      <td>0.22</td>\n      <td>0.20</td>\n      <td>0.03</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.19</td>\n      <td>0.27</td>\n      <td>0.20</td>\n      <td>0.15</td>\n      <td>0.09</td>\n      <td>0.36</td>\n      <td>0.22</td>\n      <td>0.21</td>\n      <td>0.05</td>\n      <td>0.01</td>\n      <td>...</td>\n      <td>0.09</td>\n      <td>0.22</td>\n      <td>0.32</td>\n      <td>0.08</td>\n      <td>0.36</td>\n      <td>0.32</td>\n      <td>0.45</td>\n      <td>0.42</td>\n      <td>0.40</td>\n      <td>0.16</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>...</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.09</td>\n      <td>0.10</td>\n      <td>0.22</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.18</td>\n      <td>1.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>...</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.18</td>\n      <td>0.30</td>\n      <td>0.36</td>\n      <td>0.01</td>\n      <td>0.00</td>\n      <td>0.49</td>\n      <td>1.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>...</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.27</td>\n      <td>0.55</td>\n      <td>0.51</td>\n      <td>0.06</td>\n      <td>0.08</td>\n      <td>0.99</td>\n      <td>1.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>...</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>1.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>...</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>1.00</td>\n      <td>1.00</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows Ã— 52 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_pickle('data/gan_dataprep/train_gan.pickle')\n",
    "df1.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-09T21:01:18.407117700Z",
     "start_time": "2023-05-09T21:01:17.414970600Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "devid = torch.cuda.current_device()\n",
    "print(f\"Devid: {devid}\")\n",
    "torch.cuda.set_device(devid)\n",
    "print(torch.cuda.get_device_name(devid))\n",
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(dev)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train = pd.read_pickle(\"./data/gan_dataprep/train_gan.pickle\")\n",
    "val = pd.read_pickle(\"./data/gan_dataprep/val_gan.pickle\")\n",
    "pol_dat = train\n",
    "formula = 'ClaimNb ~ VehBrand + VehGas + Region + AreaGLM + VehPower + VehAge + DrivAge + DensityGLM + BonusMalus'\n",
    "# %%\n",
    "# Wrangle train data\n",
    "td = back_from_dummies(train)\n",
    "td['ClaimNb'] = td['ClaimNb'].astype('float').astype('int')\n",
    "y_real, X_real = dmatrices(formula,\n",
    "                           data=td,\n",
    "                           return_type='dataframe')\n",
    "\n",
    "\n",
    "def xpxixpy(X, y):\n",
    "    return np.dot(np.linalg.inv(np.dot(X.T, X)), np.dot(X.T, y))\n",
    "\n",
    "\n",
    "xy = xpxixpy(X_real, y_real)\n",
    "disc_add_rows = xy.shape[0]\n",
    "\n",
    "# Fit a poisson Model\n",
    "poisson_mod = sm.GLM(y_real, X_real, family=sm.families.Poisson(), offset=td['Exposure']).fit()\n",
    "original_params = poisson_mod.params\n",
    "\n",
    "lower = poisson_mod.params - 1.96 * poisson_mod.bse\n",
    "upper = poisson_mod.params + 1.96 * poisson_mod.bse\n",
    "\n",
    "# Wrangle Test Data\n",
    "test2 = back_from_dummies(val)\n",
    "test2['ClaimNb'] = test2['ClaimNb'].astype('float').astype('int')\n",
    "y_test, X_test = dmatrices(formula,\n",
    "                           data=test2,\n",
    "                           return_type='dataframe')\n",
    "y_test_resp = np.squeeze(y_test) / np.squeeze(test2['Exposure'])\n",
    "\n",
    "# make predictions on test data with models trained on train data\n",
    "real_pois_preds = poisson_mod.predict(X_test)\n",
    "# %%"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This next section contains everything that we can tune in the GAN\n",
    "\"\"\"\n",
    "\n",
    "# Information about the size of the data\n",
    "data_size = pol_dat.shape[1]  # number of cols in pol_dat\n",
    "var_locs = [0, 1, 2, 3, 4, 5]  # tells us where the continous variables are\n",
    "\n",
    "# parameters\n",
    "z_size = 100  # how big is the random vector fed into the generator\n",
    "# we should only need the 55?\n",
    "batch_size = 200000\n",
    "temperature = None  # comes into play with the categorical activation see multioutput.py\n",
    "\n",
    "# Generator tuning\n",
    "gen_hidden_sizes = [100, 100, 100]\n",
    "gen_bn_decay = .90\n",
    "gen_l2_regularization = 0\n",
    "gen_learning_rate = 0.01\n",
    "noise_size = z_size\n",
    "output_size = [1, 1, 1, 1, 1, 1, 5, 11, 2, 22, 6]  # how many categories with in each variable\n",
    "\n",
    "assert sum(output_size) == data_size\n",
    "\n",
    "# Discriminator tuning\n",
    "disc_hidden_sizes = [data_size, data_size]\n",
    "disc_bn_decay = .90\n",
    "critic_bool = True  # if false then between 0 and 1\n",
    "mini_batch_bool = False\n",
    "disc_leaky_param = 0.2\n",
    "disc_l2_regularization = 0\n",
    "disc_learning_rate = 0.01\n",
    "penalty = 10  ## deals with gradiant penalty"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "auto_data = PolicyDataset(pol_dat, var_locs)\n",
    "auto_loader = DataLoader(auto_data,\n",
    "                         batch_size=batch_size,\n",
    "                         pin_memory=True,\n",
    "                         shuffle=True,\n",
    "                         num_workers=7\n",
    "                         )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PolicyDataset(Dataset):\n",
    "    def __init__(self, data, cont_locs, small_test = None):\n",
    "        self.policy = data.drop(data.columns[cont_locs], axis=1, inplace = False)  \n",
    "        self.cont = data.iloc[:,cont_locs]\n",
    "        self.small_test = small_test\n",
    "        self.cont_locs = cont_locs\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        if len(self.cont_locs) > 0:\n",
    "            return [torch.from_numpy(self.policy.iloc[index].values).float(),\n",
    "                  torch.from_numpy(self.cont.iloc[index].values).float()]\n",
    "        else:\n",
    "            return [torch.from_numpy(self.policy.iloc[index].values).float(),\n",
    "                  0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.iloc[:,cont_locs]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for c1, c2 in auto_loader: \n",
    "    print('ok')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "auto_loader.data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#loop = tqdm(total=epochs, position=0, leave=False)\n",
    "for epoch in range(epochs):\n",
    "    for d_epoch in range(disc_epochs):\n",
    "        print(f'Epoch: {epoch}, D-epoch {d_epoch}')\n",
    "        for c1, c2 in auto_loader:  # c1 is continous variables and c2 is the categorical variables\n",
    "            print('c1c2')\n",
    "            batch = torch.cat([c2, c1], 1)\n",
    "            batch = batch.to(dev)\n",
    "            optim_disc.zero_grad()\n",
    "\n",
    "            # train discriminator with real data\n",
    "            real_features = Variable(batch).to(dev)\n",
    "            real_pred = discriminator(real_features)\n",
    "            # the disc outputs high numbers if it thinks the data is real, we take the negative of this\n",
    "            # Because we are minimizing loss\n",
    "            real_loss = -real_pred.mean(0).view(1)\n",
    "            real_loss.backward()\n",
    "\n",
    "            # then train the discriminator only with fake data\n",
    "            noise = Variable(torch.FloatTensor(len(batch), z_size).normal_()).to(dev)\n",
    "            fake_features = generator(noise, training=True)\n",
    "            fake_features = fake_features.detach().to(dev)  # do not propagate to the generator\n",
    "            fake_pred = discriminator(fake_features)\n",
    "            fake_loss = fake_pred.mean(0).view(1)\n",
    "            fake_loss.backward()\n",
    "\n",
    "            # this is the magic from WGAN-GP\n",
    "            gradient_penalty = calculate_gradient_penalty(discriminator.to(dev), penalty, real_features.to(dev), fake_features.to(dev))\n",
    "            gradient_penalty.backward()\n",
    "\n",
    "            # finally update the discriminator weights\n",
    "            optim_disc.step()\n",
    "\n",
    "            disc_loss = real_loss + fake_loss + gradient_penalty\n",
    "            disc_losses = disc_loss.item()\n",
    "            # Delete to prevent memory leakage\n",
    "            del gradient_penalty\n",
    "            del fake_loss\n",
    "            del real_loss\n",
    "            del disc_loss\n",
    "            del real_features\n",
    "            del real_pred\n",
    "            del noise\n",
    "            del fake_features\n",
    "            del fake_pred\n",
    "\n",
    "    for g_epoch in range(gen_epochs):\n",
    "        print(f'Epoch: {epoch}, D-epoch {g_epoch}')\n",
    "        optim_gen.zero_grad()\n",
    "\n",
    "        noise = Variable(torch.FloatTensor(len(batch), z_size).normal_()).to(dev)\n",
    "        gen_features = generator(noise).to(dev)\n",
    "        gen_pred = discriminator(gen_features)\n",
    "\n",
    "        gen_loss = - gen_pred.mean(0).view(1)\n",
    "        gen_loss.backward()\n",
    "\n",
    "        optim_gen.step()\n",
    "\n",
    "        gen_loss = gen_loss\n",
    "        gen_losses = gen_loss.item()\n",
    "        del gen_loss\n",
    "        del noise\n",
    "        del gen_features\n",
    "        del gen_pred\n",
    "\n",
    "    #loop.set_description('epoch:{}, disc_loss:{:.4f}, gen_loss:{:.4f}'.format(epoch, disc_losses, gen_losses))\n",
    "    #loop.update(1)\n",
    "    # analyze poisson regression parameters every 20 epochs\n",
    "    if (epoch % 20 == 0):\n",
    "        with torch.no_grad():\n",
    "            generated_data = generator(Variable(torch.FloatTensor(pol_dat.shape[0], z_size).normal_()).to(dev), training=False)\n",
    "        df1 = pd.DataFrame(generated_data.data.to('cpu').numpy())\n",
    "        df1.columns = list(pol_dat)\n",
    "        df2 = back_from_dummies(df1)\n",
    "        df2['ClaimNb'] = df2['ClaimNb'].astype('float').astype('int')\n",
    "        y_gen, X_gen = dmatrices(formula,\n",
    "                                 data=df2,\n",
    "                                 return_type='dataframe')\n",
    "\n",
    "        # df2.to_csv(output_data_save_path)\n",
    "        # Fit poisson Model\n",
    "        try:\n",
    "            poisson_mod_gen = sm.GLM(y_gen, X_gen, family=sm.families.Poisson(), offset=np.log(df2['Exposure'])).fit()\n",
    "        except ValueError:\n",
    "            continue\n",
    "        # Calculate Errors\n",
    "        errors_pois = poisson_mod_gen.predict(X_test) - real_pois_preds\n",
    "\n",
    "        pois_metric.append(round(np.mean(errors_pois), 4))\n",
    "\n",
    "        if (epoch > 3):\n",
    "            plt.subplot(311)\n",
    "            plt.plot(pois_metric, label='train')\n",
    "            plt.ylabel('poission Dif')\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "\n",
    "        print('Mean Absolute Difference Pois:', round(np.mean(errors_pois), 2))\n",
    "\n",
    "        del errors_pois\n",
    "        del poisson_mod_gen\n",
    "        del generated_data\n",
    "        del df1\n",
    "        del df2\n",
    "        del gen_features\n",
    "\n",
    "        torch.save(generator.state_dict(), f='./saved_parameters/gen_test')\n",
    "        # print(pois_df)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, D-epoch 0\n",
      "c1c2\n",
      "c1c2\n",
      "c1c2\n",
      "c1c2\n",
      "c1c2\n",
      "Epoch: 0, D-epoch 1\n",
      "c1c2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[80], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m d_epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(disc_epochs):\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEpoch: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, D-epoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00md_epoch\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 5\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m c1, c2 \u001B[38;5;129;01min\u001B[39;00m auto_loader:  \u001B[38;5;66;03m# c1 is continous variables and c2 is the categorical variables\u001B[39;00m\n\u001B[0;32m      6\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mc1c2\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      7\u001B[0m         batch \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([c2, c1], \u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\.conda\\envs\\thesis10\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:521\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    519\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    520\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()\n\u001B[1;32m--> 521\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    522\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    523\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    524\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    525\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\.conda\\envs\\thesis10\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:561\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    559\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    560\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 561\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    562\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    563\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data)\n",
      "File \u001B[1;32m~\\.conda\\envs\\thesis10\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch\u001B[39m(\u001B[38;5;28mself\u001B[39m, possibly_batched_index):\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[1;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\.conda\\envs\\thesis10\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch\u001B[39m(\u001B[38;5;28mself\u001B[39m, possibly_batched_index):\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[1;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\thesis2\\Functions\\MC_WGAN_GP\\gan_scripts\\auto_loader.py:21\u001B[0m, in \u001B[0;36mPolicyDataset.__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m,index):\n\u001B[0;32m     20\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcont_locs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m---> 21\u001B[0m       \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_numpy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miloc\u001B[49m\u001B[43m[\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m     22\u001B[0m               torch\u001B[38;5;241m.\u001B[39mfrom_numpy(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcont\u001B[38;5;241m.\u001B[39miloc[index]\u001B[38;5;241m.\u001B[39mvalues)\u001B[38;5;241m.\u001B[39mfloat()]\n\u001B[0;32m     23\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     24\u001B[0m       \u001B[38;5;28;01mreturn\u001B[39;00m [torch\u001B[38;5;241m.\u001B[39mfrom_numpy(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpolicy\u001B[38;5;241m.\u001B[39miloc[index]\u001B[38;5;241m.\u001B[39mvalues)\u001B[38;5;241m.\u001B[39mfloat(),\n\u001B[0;32m     25\u001B[0m               \u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#loop = tqdm(total=epochs, position=0, leave=False)\n",
    "for epoch in range(epochs):\n",
    "    for d_epoch in range(disc_epochs):\n",
    "        print(f'Epoch: {epoch}, D-epoch {d_epoch}')\n",
    "        for c1, c2 in auto_loader:  # c1 is continous variables and c2 is the categorical variables\n",
    "            print('c1c2')\n",
    "            batch = torch.cat([c2, c1], 1)\n",
    "            batch = batch.to(dev)\n",
    "            optim_disc.zero_grad()\n",
    "\n",
    "            # train discriminator with real data\n",
    "            real_features = Variable(batch).to(dev)\n",
    "            real_pred = discriminator(real_features)\n",
    "            # the disc outputs high numbers if it thinks the data is real, we take the negative of this\n",
    "            # Because we are minimizing loss\n",
    "            real_loss = -real_pred.mean(0).view(1)\n",
    "            real_loss.backward()\n",
    "\n",
    "            # then train the discriminator only with fake data\n",
    "            noise = Variable(torch.FloatTensor(len(batch), z_size).normal_()).to(dev)\n",
    "            fake_features = generator(noise, training=True)\n",
    "            fake_features = fake_features.detach().to(dev)  # do not propagate to the generator\n",
    "            fake_pred = discriminator(fake_features)\n",
    "            fake_loss = fake_pred.mean(0).view(1)\n",
    "            fake_loss.backward()\n",
    "\n",
    "            # this is the magic from WGAN-GP\n",
    "            gradient_penalty = calculate_gradient_penalty(discriminator.to(dev), penalty, real_features.to(dev), fake_features.to(dev))\n",
    "            gradient_penalty.backward()\n",
    "\n",
    "            # finally update the discriminator weights\n",
    "            optim_disc.step()\n",
    "\n",
    "            disc_loss = real_loss + fake_loss + gradient_penalty\n",
    "            disc_losses = disc_loss.item()\n",
    "            # Delete to prevent memory leakage\n",
    "            del gradient_penalty\n",
    "            del fake_loss\n",
    "            del real_loss\n",
    "            del disc_loss\n",
    "            del real_features\n",
    "            del real_pred\n",
    "            del noise\n",
    "            del fake_features\n",
    "            del fake_pred\n",
    "\n",
    "    for g_epoch in range(gen_epochs):\n",
    "        print(f'Epoch: {epoch}, D-epoch {g_epoch}')\n",
    "        optim_gen.zero_grad()\n",
    "\n",
    "        noise = Variable(torch.FloatTensor(len(batch), z_size).normal_()).to(dev)\n",
    "        gen_features = generator(noise).to(dev)\n",
    "        gen_pred = discriminator(gen_features)\n",
    "\n",
    "        gen_loss = - gen_pred.mean(0).view(1)\n",
    "        gen_loss.backward()\n",
    "\n",
    "        optim_gen.step()\n",
    "\n",
    "        gen_loss = gen_loss\n",
    "        gen_losses = gen_loss.item()\n",
    "        del gen_loss\n",
    "        del noise\n",
    "        del gen_features\n",
    "        del gen_pred\n",
    "\n",
    "    #loop.set_description('epoch:{}, disc_loss:{:.4f}, gen_loss:{:.4f}'.format(epoch, disc_losses, gen_losses))\n",
    "    #loop.update(1)\n",
    "    # analyze poisson regression parameters every 20 epochs\n",
    "    if (epoch % 20 == 0):\n",
    "        with torch.no_grad():\n",
    "            generated_data = generator(Variable(torch.FloatTensor(pol_dat.shape[0], z_size).normal_()).to(dev), training=False)\n",
    "        df1 = pd.DataFrame(generated_data.data.to('cpu').numpy())\n",
    "        df1.columns = list(pol_dat)\n",
    "        df2 = back_from_dummies(df1)\n",
    "        df2['ClaimNb'] = df2['ClaimNb'].astype('float').astype('int')\n",
    "        y_gen, X_gen = dmatrices(formula,\n",
    "                                 data=df2,\n",
    "                                 return_type='dataframe')\n",
    "\n",
    "        # df2.to_csv(output_data_save_path)\n",
    "        # Fit poisson Model\n",
    "        try:\n",
    "            poisson_mod_gen = sm.GLM(y_gen, X_gen, family=sm.families.Poisson(), offset=np.log(df2['Exposure'])).fit()\n",
    "        except ValueError:\n",
    "            continue\n",
    "        # Calculate Errors\n",
    "        errors_pois = poisson_mod_gen.predict(X_test) - real_pois_preds\n",
    "\n",
    "        pois_metric.append(round(np.mean(errors_pois), 4))\n",
    "\n",
    "        if (epoch > 3):\n",
    "            plt.subplot(311)\n",
    "            plt.plot(pois_metric, label='train')\n",
    "            plt.ylabel('poission Dif')\n",
    "            plt.show()\n",
    "            plt.clf()\n",
    "\n",
    "        print('Mean Absolute Difference Pois:', round(np.mean(errors_pois), 2))\n",
    "\n",
    "        del errors_pois\n",
    "        del poisson_mod_gen\n",
    "        del generated_data\n",
    "        del df1\n",
    "        del df2\n",
    "        del gen_features\n",
    "\n",
    "        torch.save(generator.state_dict(), f='./saved_parameters/gen_test')\n",
    "        # print(pois_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}